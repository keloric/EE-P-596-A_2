{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4\n",
    "### Kyle Hadley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PCA via Successive Deflation\n",
    "\n",
    "*Note: Worked problem with Charlie and Joaquin from class on this problem.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)\n",
    "\n",
    "For the covariance of the deflated matrix, we are given the relationship $\\tilde{X}^T = (I - v_1v_1^T)X^T$. From this relationship, we know that $\\tilde{X} = X(I - v_1v_1^T)$.\n",
    "\n",
    "From these relationships, we start with our covariance of the deflated matrix such that,\n",
    "\n",
    "$$\\tilde{C} = \\frac{1}{n} \\tilde{X}^T\\tilde{X}$$\n",
    "\n",
    "Substituting our relationships for $\\tilde{X}^T$ and $\\tilde{X}$,\n",
    "\n",
    "$$\\tilde{C} = \\frac{1}{n} (I - v_1v_1^T)X^TX(I - v_1v_1^T)$$\n",
    "\n",
    "We can substitute $\\frac{1}{n}X^TX = C$ such that\n",
    "\n",
    "$$\\tilde{C} = (I - v_1v_1^T)C(I - v_1v_1^T)$$\n",
    "$$\\tilde{C} = C - v_1v_1^TC - Cv_1v_1^T + v_1v_1^TCv_1v_1^T$$\n",
    "\n",
    "Substituing our relationship $Cv_1 = \\lambda_1v_1$ and given that $\\lambda$ is a constant,\n",
    "\n",
    "$$\\tilde{C} = C - v_1v_1^TC - \\lambda_1 v_1v_1^T + v_1v_1^T\\lambda_1 v_1v_1^T$$\n",
    "$$\\tilde{C} = C - v_1v_1^TC - \\lambda_1 v_1v_1^T + \\lambda_1 v_1v_1^Tv_1v_1^T$$\n",
    "\n",
    "Also, given our orthonormality relationship we know that $v_1^Tv_1 = 1$; thus,\n",
    "\n",
    "$$\\tilde{C} = C - v_1v_1^TC - \\lambda_1 v_1v_1^T + \\lambda_1 v_1v_1^T$$\n",
    "$$\\tilde{C} = C - v_1v_1^TC$$\n",
    "\n",
    "We can take a \"double\" transpose of our left-most term (i.e. $a = (a^T)^T$); thus,\n",
    "\n",
    "$$\\tilde{C} = C - ((v_1v_1^TC)^T)^T$$\n",
    "$$\\tilde{C} = C - (C^Tv_1v_1^T)^T$$\n",
    "\n",
    "From our relationship for $C$, we can see that $C = C^T$ ($C = \\frac{1}{n}X^TX = C^T$); thus,\n",
    "\n",
    "$$\\tilde{C} = C - (Cv_1v_1^T)^T$$\n",
    "\n",
    "Applying our relationship again of $Cv_1 = \\lambda_1 v_1$ and $C = \\frac{1}{n}X^TX$,\n",
    "\n",
    "$$\\tilde{C} = \\frac{1}{n}X^TX - (\\lambda_1 v_1v_1^T)^T$$\n",
    "$$\\tilde{C} = \\frac{1}{n}X^TX - \\lambda_1 v_1v_1^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)\n",
    "\n",
    "For a $j \\neq 1$ and $v_j$ as a principle eigvenvector of $C$, we can show that $v_j$ is also a principal eigenvector of $\\tilde{C}$ with the same eigenvalue $\\lambda_j$.\n",
    "\n",
    "Given $\\tilde{C} = \\frac{1}{n}X^TX - \\lambda_1 v_1v_1^T$, we right-multiply both sides by $v_j$,\n",
    "\n",
    "$$\\tilde{C}v_j = \\left(\\frac{1}{n}X^TX - \\lambda_1 v_1v_1^T\\right) v_j$$\n",
    "$$\\tilde{C}v_j = \\frac{1}{n}X^TXv_j - \\lambda_1 v_1v_1^Tv_j$$\n",
    "\n",
    "Given our orthonormality relationship, we know that if $j \\neq 1$ then $v_1^Tv_j = 0$; thus,\n",
    "\n",
    "$$\\tilde{C}v_j = \\frac{1}{n}X^TXv_j - \\lambda_1 v_1(0)$$\n",
    "$$\\tilde{C}v_j = \\frac{1}{n}X^TXv_j$$\n",
    "\n",
    "Substituting $\\frac{1}{n}X^TX = C$,\n",
    "\n",
    "$$\\tilde{C}v_j = Cv_j = \\lambda_j v_j$$\n",
    "\n",
    "Thus, we can see that $v_j$ is also a principal eigenvector of $\\tilde{C}$ with the same eigenvalue $\\lambda_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)\n",
    "\n",
    "By deflating our matrix $C$ to $\\tilde{C}$, we are essentially removing one of the parameters from our initial dataset as determined by the principal eigenvector - which was given as $v_1$. Since we know that we have removed the parameter associated with $v_1$ when creating our deflated covariance matrix, we know that $v_1$ is no longer the first principal eigenvector of $\\tilde{C}$. Thus, $v_2$ must be the first principal eigenvector for $\\tilde{C}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d)\n",
    "\n",
    "Intuitively, when we multiply our vector $u_0$ by our covariance matrix $C$, we end up stretching our $u_0$ along the eigenvectors of $C$. However, the stretching is greatest along the principal eigenvector direction.\n",
    "\n",
    "Thus, if we repeat this process of stretching for a large number of times ($k$ times), we will get a resulting $u_k$ will look like a stretched version of our principal eigenvector. We can then see that $u_k$ is just a scaled version of the principal eigenvector - thus meaning that they are equivalent.\n",
    "\n",
    "The computational complexity is $O(m^2k)$ as we are performing a matrix multiplication of $C$ by itself (which is a $O(m)$ operation) $k$ times.\n",
    "\n",
    "It would be advantageous to include the norming component as referenced in eq. (5) because this avoids our values increasing or decreasing exponentially as we stretch $u_0$. This ensures that our value is within the memory limits of computers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues:\n",
      " Predicted: [4.14066235 4.0241905  0.89120969 0.55592762 0.04734749] \n",
      " Actual: [4.14066235 2.01209525 0.02367374 0.27796381 0.44560484]\n",
      "\n",
      "Eigenvectors:\n",
      " Predicted:\n",
      " [[ 0.42723111 -0.51084397  0.15881388  0.61225951  0.39551039]\n",
      " [ 0.3670085   0.52528157  0.12882228  0.50999197 -0.5591932 ]\n",
      " [ 0.59423189 -0.41956691  0.1213297  -0.51070685 -0.44193855]\n",
      " [ 0.33912602  0.47707352  0.54570421 -0.31125022  0.51256669]\n",
      " [ 0.46331452  0.24388967 -0.80353531 -0.08572222  0.26988923]] \n",
      " Actual:\n",
      " [[-0.42723111 -0.51084397  0.39551039  0.61225951 -0.15881388]\n",
      " [-0.3670085   0.52528157 -0.5591932   0.50999197 -0.12882228]\n",
      " [-0.59423189 -0.41956691 -0.44193855 -0.51070685 -0.1213297 ]\n",
      " [-0.33912602  0.47707352  0.51256669 -0.31125022 -0.54570421]\n",
      " [-0.46331452  0.24388967  0.26988923 -0.08572222  0.80353531]]\n"
     ]
    }
   ],
   "source": [
    "def calculate_eigens(C, k):\n",
    "    n, m = C.shape\n",
    "\n",
    "    u0 = np.ones(n)\n",
    "    u = np.linalg.matrix_power(C, k).dot(u0)\n",
    "    \n",
    "    u = u / np.linalg.norm(u)\n",
    "\n",
    "    lamda = C.dot(u)[0] / u[0]\n",
    "    return lamda, u.T\n",
    "\n",
    "def calculate_K_eigens(C, X, K, k):\n",
    "    n, m = C.shape\n",
    "\n",
    "    lamda = np.zeros(K)\n",
    "    v = np.zeros(shape=(n, K))\n",
    "\n",
    "    for i in range(K):\n",
    "        lamda[i], v[:, i] = calculate_eigens(C, k)\n",
    "        u = np.expand_dims(v[:, i], axis=1)\n",
    "        X = X.dot(np.identity(n) - u.dot(u.T))\n",
    "        C = 1/n * (X.T).dot(X)\n",
    "\n",
    "    #print(C)\n",
    "\n",
    "    return lamda, v\n",
    "\n",
    "n = 10\n",
    "m = 5\n",
    "np.random.seed(0)\n",
    "X = np.random.randint(3, size=(n, m))\n",
    "#print(X)\n",
    "C = 1/n * (X.T).dot(X)\n",
    "\n",
    "l, v = calculate_K_eigens(C, X, 5, 50)\n",
    "l_true, v_true = np.linalg.eig(C)\n",
    "\n",
    "print('Eigenvalues:\\n', 'Predicted:', l, '\\n', 'Actual:', l_true)\n",
    "print('\\nEigenvectors:\\n', 'Predicted:\\n', v, '\\n', 'Actual:\\n', v_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Locality Sensitive Hashing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)\n",
    "\n",
    "In order to avoid the 3rd condition where the magical oracle is unstable, we set $c = 1$ such that $r = cr$. This will ensure that we avoid the unstable condition when requesting a nearest neighbor.\n",
    "\n",
    "After setting $c=1$, we can perform iterative queries by starting with an arbitrary $r_0$. Given an $r_0$ we can ask the oracle for the nearest neighbor of a query point $q$.\n",
    "\n",
    "If the oracle returns a point $x'$, then we will reduce our $r_0$ by half (rounded down) and perform a new query with our new $r_1$ (i.e. half the original $r_0$). If the oracle still returns a point $x'$, then we continue to reduce our $r_i$ by half (as long as it's possible with integer values) until we've reached a point where the oracle finds nothing. Once the oracle finds nothing, then we take the average of our $r_{i-1}$ from the previous iteration and the latest $r_i$ for $r_{i+1}$. We perform this \"yo-yo\"-ing until we reach a point where we know that there is no smaller $r$ that yields an $x'$.\n",
    "\n",
    "We perform a similar series of steps if the oracle doesn't return a point $x'$ with our initial $r_0$ but instead set $r_{i+1} = 2r_i$ until the oracle does return an $x'$. Then we begin the same \"yo-yo\"ing as described above until we reach a set $r$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)\n",
    "\n",
    "The lower bound for the probability that $h$ maps $x_i$ and $x_j$ to the same value is a function of the number of common values within $x_i$ and $x_j$ at the same indices. While we don't have a function or value for the common values, we can use the Hamming distance as this represents where $x_i$ and $x_j$ have differing values at various positions within the vectors.\n",
    "\n",
    "Given that $d(x_i, x_j) \\leq r$, we know that the lower bound for common values (i.e. $h(x_i) = h(x_j)$) is when $d(x_i, x_j) = r$; thus, we can describe the probability of $h(x_i) = h(x_j)$ as,\n",
    "\n",
    "$$p_1 = Pr(h(x_i) = h(x_j)) = 1 - \\frac{r}{m}$$\n",
    "\n",
    "where $m$ is the size of the binary vectors $x_i$ and $x_j$.\n",
    "\n",
    "Given $d(x_i, x_j) \\geq r$, we know that upper bound for common values is when $d(x_i, x_j) = cr$; thus, we can describe the probability of $h(x_i) = h(x_j)$ as,\n",
    "\n",
    "$$p_2 = Pr(h(x_i) = h(x_j)) = 1 - \\frac{cr}{m}$$\n",
    "\n",
    "The inequality relationship between $p_1$ and $p_2$ is $p_2 \\leq p_1$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)\n",
    "\n",
    "Given that $g(x_i) = (h_1(x_i), h_2(x_i), \\ldots , h_k(x_i))$, we can surmise that the probability of $g(x_i) = g(x_j)$ is equivalent to the combined probability of $h_1(x_i) = h_1(x_j)$, $h_2(x_i) = h_2(x_j)$, ..., $h_k(x_i) = h_k(x_j)$; thus,\n",
    "\n",
    "$$Pr(g(x_i) = g(x_j)) = Pr(h_1(x_i) = h_1(x_j)) \\times Pr(h_2(x_i) = h_2(x_j)) \\times \\ldots \\times Pr(h_k(x_i) = h_k(x_j))$$\n",
    "\n",
    "Substituting our $p_1$ for our lower bound condition, the lower bound of our probability is\n",
    "\n",
    "$$Pr(g(x_i) = g(x_j)) = p_1 \\times p_1 \\times \\ldots \\times p_1$$\n",
    "$$Pr(g(x_i) = g(x_j)) = p_1^k$$\n",
    "\n",
    "Substituting our $p_2$ for our lower bound condition, the lower bound of our probability is\n",
    "\n",
    "$$Pr(g(x_i) = g(x_j)) = p_2 \\times p_2 \\times \\ldots \\times p_2$$\n",
    "$$Pr(g(x_i) = g(x_j)) = p_2^k$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d)\n",
    "\n",
    "To find the probability that for $b \\in \\{0, 1, ..., l-1\\}$ where $g_b(x_i) = g_b(x_j)$, we can consider the probability that this will never occur such that,\n",
    "\n",
    "$$Pr\\left(\\exists b \\in \\{0, 1,..., l-1 \\} : g_b(x_i) = g_b(x_j) \\right) = 1 - Pr\\left(g_b(x_i) \\neq g_b(x_j) , \\forall b\\right)$$\n",
    "\n",
    "We know from part (c) that the probability of $Pr(g(x_i) = g(x_j)) = p_1^k$ for lower and upper bounds. Thus, we know that for $Pr(g(x_i) \\neq g(x_j)) = 1 - p_1^k$. Given that we are calculating the probability across all values of $b$, we can then surmise that,\n",
    "\n",
    "$$Pr\\left(g_b(x_i) \\neq g_b(x_j), \\forall b\\right) = (1 - p_1^k)^l$$\n",
    "\n",
    "Thus, for the lower bound we know that\n",
    "\n",
    "$$Pr\\left(\\exists b \\in \\{0, 1,..., l-1 \\} : g_b(x_i) = g_b(x_j) \\right) = 1 - (1 - p_1^k)^l$$\n",
    "\n",
    "Similarly, for the upper bound we know that\n",
    "\n",
    "$$Pr\\left(\\exists b \\in \\{0, 1,..., l-1 \\} : g_b(x_i) = g_b(x_j) \\right) = 1 - (1 - p_2^k)^l$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e)\n",
    "\n",
    "Given that $\\rho = \\frac{\\ln(p_1)}{\\ln(p_2)}$, $l = n^\\rho$, and $k = \\frac{\\ln(n)}{\\ln(1/p_2)}$.\n",
    "\n",
    "Solving the probability of the first event, given that $d(x', q) \\leq r$ we know that we are dealing with the lower bound probability as defined in part (d). Thus, we know that,\n",
    "\n",
    "$$Pr(A) = 1 - (1 - p_1^k)^l$$\n",
    "\n",
    "where $P(A)$ is the probability of the first event. First substituting our relationship for $k$ and $l$, we find that\n",
    "\n",
    "$$Pr(A) = 1 - \\left(1 - p_1^{\\frac{\\ln(n)}{\\ln(1/p_2)}} \\right)^{n^\\rho}$$\n",
    "\n",
    "Examining the term $p_1^{\\frac{ln(n)}{\\ln(1/p_2)}}$, we can re-write this as\n",
    "\n",
    "$$p_1^{\\ln(n)^{\\frac{1}{\\ln(1/p_2)}}}$$\n",
    "\n",
    "We can apply the log base change rule such that $\\ln(n) = \\frac{\\log_{p_1}n}{\\log_{p_1}e}$; thus,\n",
    "\n",
    "$$p_1^{\\frac{\\log_{p_1}n}{\\log_{p_1}e}^{\\frac{1}{\\ln(1/p_2)}}}$$\n",
    "\n",
    "We know that $p_1^{\\log_{p_1}n} = n$; thus,\n",
    "\n",
    "$$n^{\\frac{1}{\\log_{p_1}e}^{\\frac{1}{\\ln(1/p_2)}}}$$\n",
    "\n",
    "Again, applying the log base change rule it is known that $\\frac{1}{\\log_{p_1}(e)} = \\frac{\\ln(p_1)}{\\ln(e)} = \\ln (p_1)$; thus,\n",
    "\n",
    "$$n^{\\ln (p_1)^{\\frac{1}{\\ln(1/p_2)}}} = n^{\\frac{\\ln (p_1)}{\\ln(1/p_2)}} = \\frac{1}{n^{\\frac{\\ln (p_1)}{\\ln(p_2)}}}$$\n",
    "\n",
    "We know from our relationship for $\\rho$ that,\n",
    "\n",
    "$$\\frac{1}{n^{\\frac{\\ln (p_1)}{\\ln(p_2)}}} = \\frac{1}{n^\\rho}$$\n",
    "\n",
    "Re-plugging this new value into our initial equation for $Pr(A)$ we see that,\n",
    "\n",
    "$$Pr(A) = 1 - \\left(1 - \\frac{1}{n^\\rho} \\right)^{n^\\rho}$$\n",
    "\n",
    "As highlighted by the hint in the homework, we know that for an arbitrary value, $z$, the $\\lim_{k \\rightarrow \\infty} (1-1/z)^z = e^{-1}$ and that $(1-1/z)^z$ will always be less than $e^{-1}$. Thus, we can surmise that the $Pr(A)$ can be re-written as,\n",
    "\n",
    "$$Pr(A) \\geq 1 - e^{-1}$$\n",
    "\n",
    "Thus, we see that the first event happens with probability of at least $1-e^{-1}$.\n",
    "\n",
    "Solving the probability of the second event, we can define our probability as,\n",
    "\n",
    "$$Pr(B) = Pr(W \\leq 4l)$$\n",
    "\n",
    "where $Pr(B)$ is the probability of the second event, $W$ is our random variable of the number of items in $X$. We can re-write this such that\n",
    "\n",
    "$$Pr(B) = 1 - Pr(W \\geq 4l)$$\n",
    "\n",
    "From this relationship, we apply Markov's inequality such that,\n",
    "\n",
    "$$Pr(B) \\geq 1 - \\frac{E[W]}{4l}$$\n",
    "\n",
    "where $E(W)$ represents the expectation of $W$ defined as $E[W] = \\sum x_i p_i$. The probability in this relationship is $p_i = l * Pr(g_b(x) = g_b(q))$ for a given $b$, as defined by part (c), $p^k$. We know that we are dealing with the upper limit, i.e. $p_2$, because $d(x,q) \\geq cr$. Thus, we first simplifying our $p_i = l * p_2^k$,\n",
    "\n",
    "$$p_i = l * p_2^k = l * p_2^{\\frac{\\ln(n)}{\\ln(1/p_2)}} = l * p_2^{-\\frac{\\ln(n)}{\\ln(p_2)}}$$\n",
    "\n",
    "We can apply the log base change rule such that $\\frac{\\ln(n)}{\\ln(p_2)} = \\log_{p_2}(n)$; thus,\n",
    "\n",
    "$$l * p_2^{-\\frac{\\ln(n)}{\\ln(p_2)}} = l * p_2^{-\\log_{p_2}(n)} = l* n^{-1} = l * \\frac{1}{n}$$\n",
    "\n",
    "Thus, we know that probability of there being a single $x$ and $q$ where $p_i = l * \\frac{1}{n}$. We know that this probability is equivalent for all values of $W$; thus, we can re-write our expectation as,\n",
    "\n",
    "$$E[W] = \\sum x_i p_i = n p = n\\left(\\frac{l}{n}\\right) = l$$\n",
    "\n",
    "Plugging this value into our $Pr(B)$ equation, we see that,\n",
    "\n",
    "$$Pr(B) \\geq 1 - \\frac{l}{4l}$$\n",
    "$$Pr(B) \\geq \\frac{3}{4}$$\n",
    "\n",
    "Thus, we see that the probability of the second event is at least $3/4$.\n",
    "\n",
    "The lower bound on the probability of both events happening is defined by\n",
    "\n",
    "$$Pr(A \\cap B) \\geq Pr(A) * Pr(B)$$\n",
    "\n",
    "using the lower bound of both 1st and 2nd events. Thus,\n",
    "\n",
    "$$Pr(A \\cap B) \\geq \\frac{3}{4}(1-e^{-1})$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (f)\n",
    "\n",
    "In order to return a point that has a distance $cr$ to the query point $q$, we need to check "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Programming Problem: Random Forests\n",
    "\n",
    "*Note: Worked on this problem with Niraj and Charlie from class on problems 3 and 4.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)\n",
    "\n",
    "Reference *GBDT.ipynb* for coding implementation of Random Forest code.\n",
    "\n",
    "Given the Boston house price dataset, below in the table is the calculated training and test RMSE using the Random Forest object. In addition, the table also included the results from least square regression and ridge regression as calculated in previous homeworks.\n",
    "\n",
    "| ML Technique | RMSE (Training Data) | RMSE (Test Data) |\n",
    "| :-: | :-: | :-: |\n",
    "| Random Forest (RF) | 2.131 | 3.343 |\n",
    "| Least Square Regression | 4.8206 | 5.2092 |\n",
    "| Ridge Regression | 4.8419 | 5.1474 |\n",
    "\n",
    "We can see that the Random Forest performs significantly better than both least square and ridge regression based on RMSE score for the training dataset and performs better for the test dataset.\n",
    "\n",
    "The plots below demonstrate the performance of RF vs. the other regression solutions. The plots below demonstrate how close the predicted value is to the actual value for both our training and test datasets. The line in each graph represents a perfect correlation (i.e. if prediction was equal to actual) and the scattered dots represent the predictions with respect to actuals.\n",
    "\n",
    "![Problem_3_a](Problem_3_a_1.png)\n",
    "![Problem_3_a](Problem_3_a_2.png)\n",
    "![Problem_3_a](Problem_3_a_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)\n",
    "\n",
    "Reference *GBDT.ipynb* for coding implementation of Random Forest code.\n",
    "\n",
    "Given the Credit-g and Brest Cancer Diagnostic datasets, below in the table is the calculated training and test accuracy using the Random Forest object.\n",
    "\n",
    "| Dataset | Accuracy (Training Data) | Accuracy (Test Data) |\n",
    "| :-: | :-: | :-: |\n",
    "| Credit-g | 90.71 % | 78.33 % |\n",
    "| Brest Cancer Diagnostic | 98.99 % | 97.66 % |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Programming Problem: Gradient Boosting Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)\n",
    "\n",
    "The computational complexity of optimizing a tree of depth $d$ in terms of $m$ and $n$ is $O(nmd)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)\n",
    "\n",
    "It seems that the most computational expensive part of the GBDT training is the building of each individual tree.\n",
    "\n",
    "Besides performing parallel programming, we could attempt to optimize the tree building by identifying which features are most likely to drive the division of training data. For example, we may know that feature $j$ has only two values - thus, we can infer that our dataset should be split along this feature immediately (assuming that feature $j$ has a non-negligble impact on our calculated value).\n",
    "\n",
    "There is also a technique called decision tree pruning in which we remove from the decision tre sections of the tree that are non-critical and/or redundant. For example, we may perform a split on our root node by feature $j$ and then perform a split on the left child again by feature $j$. We may be able to prune some of the nodes from the tree that exhibit this type of behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)\n",
    "\n",
    "The recursive calling of left and right child can be performed in parallel as the generation of the children nodes have independent inputs / outputs from each other. Thus, we can be generating the left child (and subsequent children) in parallel to the right child."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d)\n",
    "\n",
    "One way in which GBDT is different from other gradient descent algorithims and approaches is the way in which the model updates itself. With gradient descent, we descend the differentiable loss function by introducing changes to our parameters (the weights we calculate). For GBDT, rather than changing the parameters we instead descend the gradient by introducing new models - the is similar to updating our weights but slightly different.\n",
    "\n",
    "Another difference is the way in which GBDT updates the \"parameters\" (i.e. the way it adds a model). Gradient descent calculates it updates with respect to each individual value of our training set while GBDT updates with respect to a very small subset of our training set at each leaf node (dependent on the value of our *min_sample_split*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e)\n",
    "\n",
    "Reference *GBDT.ipynb* for coding implementation of Random Forest code.\n",
    "\n",
    "Given the Boston house price dataset, below in the table is the calculated training and test RMSE using the GBDT object. In addition, the table also included the results from least square regression and ridge regression as calculated in previous homeworks.\n",
    "\n",
    "| ML Technique | RMSE (Training Data) | RMSE (Test Data) |\n",
    "| :-: | :-: | :-: |\n",
    "| GBDT | 0.568 | 3.597 |\n",
    "| Least Square Regression | 4.8206 | 5.2092 |\n",
    "| Ridge Regression | 4.8419 | 5.1474 |\n",
    "\n",
    "We can see that the GBDT performs significantly better than both least square and ridge regression based on RMSE score for the training dataset and performs better for the test dataset. The extremely low RMSE value for the training data, but larger RMSE for test data, indicates that the model may be tuned too much to the training data. However, the model as captured in code seems to optimize the test data RMSE as best as possible (barring small optimization tweaks).\n",
    "\n",
    "The plot below demonstrates the performance of GBDT vs. the other regression solutions. The plot demonstrates how close the predicted value is to the actual value for both our training and test datasets. The line in each graph represents a perfect correlation (i.e. if prediction was equal to actual) and the scattered dots represent the predictions with respect to actuals. This plot can be compared with the other plots for least square and ridge regression and presented in 3(a).\n",
    "\n",
    "![Problem_4_e](Problem_4_e_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (f)\n",
    "\n",
    "Reference *GBDT.ipynb* for coding implementation of Random Forest code.\n",
    "\n",
    "Given the Credit-g and Brest Cancer Diagnostic datasets, below in the table is the calculated training and test accuracy using the GDBT object.\n",
    "\n",
    "| Dataset | Accuracy (Training Data) | Accuracy (Test Data) |\n",
    "| :-: | :-: | :-: |\n",
    "| Credit-g | 96.57 % | 78.00 % |\n",
    "| Brest Cancer Diagnostic | 99.75 % | 95.32 % |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (g)\n",
    "\n",
    "It seems that across all three experiments that GBDT was signficantly more accurate for the training dataset classification or calculation, but was roughly equivalent to the accuracy for the test dataset.\n",
    "\n",
    "A few possible explanations:\n",
    "- The inputted parameters for both RF and GDBT caused the models to overfit to the training datasets. I attempted to optimize my models based on test dataset accuracy, without significantly causing lower accuracy on my training dataset (i.e. fudging the numbers to get a good test dataset accuracy).\n",
    "- The size of the dataset may be indicative of which model may prove more accurate. If GDBT was more effective at classifying or predicting for my training dataset, it seems that having a larger dataset that I can train on would help the model be more indicative of my real dataset (outside of my training/test data)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}