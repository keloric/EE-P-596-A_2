{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2\n",
    "### Kyle Hadley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Ridge Regression\n",
    "\n",
    "Given our equation for linear regression of $min_w \\left\\|Xw-y\\right\\|^2_2$ and our equation for ridge regression of $min_w \\left\\|Xw-y\\right\\|^2_2 + \\frac{\\eta}{2}\\left\\|w\\right\\|^2_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)\n",
    "\n",
    "For parts (a) and (b) I've generated a dataset $(X, y)$ such that the training data is equivalent across both examples, but the test data (and thus the entire dataset of $X$) varies.\n",
    "\n",
    "Given the dataset as shown below, we can see that in this instance the linear regression solution is preferred for our dataset $X$ than the ridge regression model.\n",
    "\n",
    "![Problem 1(a) Graphic](Problem_1_a_graphic.png)\n",
    "\n",
    "In this situation, while the linear regression solution has a high slope due to the nature of our training data, the overfitting to the training data matches (low variance) with the total dataset which includes the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)\n",
    "\n",
    "Given the dataset as shown below, we can see that in this instance the ridge regression solution is preferred for our dataset $X$ than the linear regression model.\n",
    "\n",
    "![Problem 1(a) Graphic](Problem_1_a_graphic.png)\n",
    "\n",
    "In this situation, the linear regression is too biased towards our initial training data and overfits with respect to this training data. The ridge regression solution provides a bias that attempts to resolve the issue of overfitting to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)\n",
    "\n",
    "The regularization hyper-parameter $\\eta$ impacts the fit of the optimization model for a given problem. If $\\eta$ is significantly large, then the model will underfit resulting in high bias and low variance. In contrast, if $\\eta$ is small, then the model will overfit resulting in a low bias and high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d)\n",
    "\n",
    "To solve the closed form solution, we can set the gradient of our objective function $F(w)$ equal to zero and then solve for $w$. We can write our $F(w)$ as,\n",
    "\n",
    "$$F(w) = \\left\\|Xw-y\\right\\|^2_2 + \\frac{\\eta}{2}\\left\\|w\\right\\|^2_2 = \\frac{1}{n}(Xw-y)^T(Xw-y) + \\frac{\\eta}{2n}w^Tw$$\n",
    "\n",
    "Taking the derivative of $F(w)$ and setting it equal to zero we get the relationship,\n",
    "\n",
    "$$\\frac{\\partial F(w)}{\\partial w} = 0 = \\frac{2}{n}X^T(Xw-y) + \\frac{\\eta}{n}w$$\n",
    "\n",
    "Now solving for $w$ we can find our closed form solution.\n",
    "\n",
    "$$0 = \\frac{2}{n}X^T(Xw-y) + \\frac{\\eta}{n}w$$\n",
    "$$0 = X^T(Xw-y) + \\frac{\\eta}{2}w$$\n",
    "$$0 = X^TXw-X^Ty + \\frac{\\eta}{2}w$$\n",
    "$$w\\left(X^TX + \\frac{\\eta}{2}\\right) = X^Ty$$\n",
    "$$w = \\left(X^TX + \\frac{\\eta}{2}\\right)^{-1}X^Ty$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e)\n",
    "\n",
    "(1) When the columns of $X$ are more than the rows, i.e. $m > n$, we are unable to compute the closed-form solution of the vanilla linear regression because the term $X^TX$ will no longer be invertible. Thus, we will unable to solve for our weights ($w$). However, in contrast the ridge regression solution can be found.\n",
    "\n",
    "(2) When the columns of $X$ are highly correlated, we are unable to compute a closed-form solution of the vanilla linear regression. If we have two variables that are nearly identical (or identical in the extreme case) we are unable to generate a linear model that can account for both of these as inputs to our output $y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (f)\n",
    "\n",
    "Given,\n",
    "\n",
    "$$F(w, w_0) = \\left\\|Xw+w_0-y\\right\\|^2_2 + \\frac{\\eta}{2}\\left\\|w\\right\\|^2_2 = \\frac{1}{n}(Xw+w_0-y)^T(Xw+w_0-y) + \\frac{\\eta}{2n}w^Tw$$\n",
    "\n",
    "First, we can take the derivative of $F(W)$ with respect to $w_0$ to solve for $w_0$ and set equal to zero.\n",
    "\n",
    "$$\\frac{\\partial F(w, w_0)}{\\partial w_0} = 0 = \\frac{2}{n}(Xw+w_0-y) $$\n",
    "$$0 = (Xw+w_0-y)$$\n",
    "$$w_0 = y - Xw$$\n",
    "\n",
    "Taking the derivative of $F(w)$ and setting it equal to zero we get the relationship,\n",
    "\n",
    "$$\\frac{\\partial F(w)}{\\partial w} = 0 = \\frac{2}{n}X^T(Xw+w_0-y) + \\frac{\\eta}{n}w$$\n",
    "\n",
    "Now solving for $w$ with a substitution for $w_0$ we can find our closed form solution.\n",
    "\n",
    "$$0 = \\frac{2}{n}X^T(Xw+(y-Xw)-y) + \\frac{\\eta}{n}w$$\n",
    "$$0 = \\frac{2}{n}X^TI + \\frac{\\eta}{n}w$$\n",
    "$$0 = X^T + \\frac{\\eta}{2}w$$\n",
    "$$w = \\frac{2}{\\eta}X^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bias and Variance\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Programming Problem: Linear Algebra\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)\n",
    "\n",
    "By observation, it seems that the most correlated features to the house price (MEDV) are LSTAT, RM and B.\n",
    "\n",
    "*Note: As discussed in part (b), we can see that actually the most  correlated features are RM, LSTAT, and PTRATIO.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b)\n",
    "\n",
    "On reviewing the heatmap of seaborn, the most correlated features to the house price (MEDV) are RM, LSTAT, and PTRATIO - which do not directly align with what was observed initially in part (a). However, this misalignment is noted in part(a)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c)\n",
    "\n",
    "After running both the linear and ridge regression on the randomly train-test split training set, these are the obtained coefficients  $w$.\n",
    "\n",
    "*Note: The reported coefficients for ridge regression used an $\\eta = 15$.*\n",
    "\n",
    "| Coefficients ($w$) | Linear | Ridge |\n",
    "| :-: | :-: | :-: |\n",
    "| CRIM |-0.099324| -0.104002|\n",
    "|        ZN  |0.052251| 0.052225|\n",
    "|     INDUS | 0.004516| 0.037553|\n",
    "|      CHAS | 2.957261| 2.689784|\n",
    "|       NOX | 1.127938| -5.698645|\n",
    "|        RM | 5.854198| 6.144848|\n",
    "|       AGE |-0.014957|-0.008247|\n",
    "|       DIS |-0.920844| -0.989810|\n",
    "|       RAD | 0.159519|0.163495|\n",
    "|      TAX |-0.008934|-0.007848|\n",
    "|  PTRATIO |-0.435674|-0.424732|\n",
    "|        B | 0.014905|0.015801|\n",
    "|    LSTAT |-0.474751|-0.443902|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d)\n",
    "\n",
    "After calculating the RMSE for both Linear and Ridge regression, against both training and test dataset, the resulting RMSE values are outputted in the table below.\n",
    "\n",
    "|  | Linear | Ridge |\n",
    "| :-: | :-: | :-: |\n",
    "| Training Set |4.8206| 4.8419|\n",
    "| Test Set |5.2092| 5.1474|\n",
    "\n",
    "We can see that the RMSE for the linear regression is lower than the RMSE for ridge regression when evaluating against the training set, but is higher for the test set. It could be likely that given these results, the linear regression model is overfitting the training data and thus has a higher error when we evaluate against a test set. While the ridge regression model is not as accurate for the given training set, we can see that it is more accurate when evaluated aginst a dataset not used for training.\n",
    "\n",
    "In both cases, the RMSE is greater for the test set than the training set. This may be expected because models will inherently be more accurate within the training dataset, as the training dataset is what is used to calculate our weights within the model. Inherently, a model will natural \"overfit\" to the training data - the goal is strike a balance between over and underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (e)\n",
    "\n",
    "After training both a linear and ridge regression model against only the top-3 features, we get the following RMSE values as shown in the table below.\n",
    "\n",
    "|  | Linear | Ridge |\n",
    "| :-: | :-: | :-: |\n",
    "| Training Set |5.2734| 5.2739|\n",
    "| Test Set |5.4947| 5.4880|\n",
    "\n",
    "Given the results, we can see that using only the top-3 features results in higher RMSE values in all aspects (linear v. ridge and training v. test). The difference between the RMSE value isn't insignificant (an increase in ~10% error), but the increase may be considered reasonable in order to reduce the complexity of the model. Being able to reduce the number of inputs for a model from 13 to 3 could significantly increase computational efficiency when the dataset expands or other limitations exist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Programming Problem: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a)\n",
    "\n",
    "Given $F(W) = \\frac{1}{n} \\sum\\limits_{i=1}^{n} -\\log\\left[Pr(y=y_i | x=X_i; W)\\right] + \\frac{\\eta}{2} \\|W\\|_F^2$, we can solve for our gradient, i.e. $\\frac{\\partial F(W)}{\\partial W}$.\n",
    "\n",
    "We can first re-write our loss function as the following,\n",
    "\n",
    "$$F(W) = -\\frac{1}{n}\\sum\\limits_{i=1}^{n} \\left[y_n * \\log(e^{z}) - \\log\\left(\\sum_{j=1}^c e^{z_j}\\right)\\right] + \\frac{\\eta}{2} \\|W\\|_F^2$$\n",
    "$$F(W) = \\frac{1}{n}\\sum\\limits_{i=1}^{n} \\left[-y_n * z + \\log\\left(\\sum_{j=1}^c e^{z_j}\\right)\\right] + \\frac{\\eta}{2} \\|W\\|_F^2$$\n",
    "\n",
    "\n",
    "\n",
    "Now we can solve for our derivative such that\n",
    "\n",
    "$$\\frac{\\partial F(W)}{\\partial W} = \\sum\\limits_{i=1}^{n} \\left[-y_n * X_n + \\frac{e^{z}}{\\sum_{j=1}^c e^{z_j}}X_n\\right] + \\eta W$$\n",
    "$$\\frac{\\partial F(W)}{\\partial W} = \\sum\\limits_{i=1}^{n} \\left[ X_n (Pr(y = k | X; W) - y_n ) \\right] + \\eta W$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
